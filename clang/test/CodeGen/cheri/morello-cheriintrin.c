// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 %s -triple aarch64-none-elf -target-feature +morello -o - \
// RUN:    -emit-llvm -O1 -Weverything -Werror -verify | FileCheck %s --check-prefix=A64
// RUN: %clang_cc1 %s -triple aarch64-none-elf -target-feature +morello \
// RUN:    -target-feature +c64 -target-abi purecap -o - \
// RUN:    -emit-llvm -O1 -Weverything -Werror -verify | FileCheck %s --check-prefix=C64
// expected-no-diagnostics

// Modified from the cheriiintrin.c test.

#include <cheriintrin.h>
// Check that all macros defined in cheriintrin.h work as expected

void use_size_t(__SIZE_TYPE__ s);
void use_bool(_Bool b);
void use_cap(void *__capability p);
void test(void *__capability cap, char *__capability cap2, void *ptr, __SIZE_TYPE__ i);
void test_alignment_builtins(void *__capability cap, __SIZE_TYPE__ align);

// A64-LABEL: define {{[^@]+}}@test
// A64-NEXT:  entry:
// A64-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CAP:%.*]])
// A64-NEXT:    call void @use_size_t(i64 [[TMP0]]) #5
// A64-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CAP]], i64 [[I:%.*]])
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP1]]) #5
// A64-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.base.get.i64(i8 addrspace(200)* [[CAP]])
// A64-NEXT:    call void @use_size_t(i64 [[TMP2]]) #5
// A64-NEXT:    [[TMP3:%.*]] = call i64 @llvm.cheri.cap.length.get.i64(i8 addrspace(200)* [[CAP]])
// A64-NEXT:    call void @use_size_t(i64 [[TMP3]]) #5
// A64-NEXT:    [[TMP4:%.*]] = call i64 @llvm.cheri.cap.offset.get.i64(i8 addrspace(200)* [[CAP]])
// A64-NEXT:    call void @use_size_t(i64 [[TMP4]]) #5
// A64-NEXT:    [[TMP5:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* [[CAP]], i64 [[I]])
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP5]]) #5
// A64-NEXT:    [[TMP6:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.tag.clear(i8 addrspace(200)* [[CAP]])
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP6]]) #5
// A64-NEXT:    [[TMP7:%.*]] = call i1 @llvm.cheri.cap.tag.get(i8 addrspace(200)* [[CAP]])
// A64-NEXT:    call void @use_bool(i1 [[TMP7]]) #5
// A64-NEXT:    call void @use_bool(i1 [[TMP7]]) #5
// A64-NEXT:    [[LNOT:%.*]] = xor i1 [[TMP7]], true
// A64-NEXT:    call void @use_bool(i1 [[LNOT]]) #5
// A64-NEXT:    [[TMP8:%.*]] = call i1 @llvm.cheri.cap.subset.test(i8 addrspace(200)* [[CAP]], i8 addrspace(200)* [[CAP2:%.*]])
// A64-NEXT:    call void @use_bool(i1 [[TMP8]]) #5
// A64-NEXT:    [[TMP9:%.*]] = call i64 @llvm.cheri.round.representable.length.i64(i64 [[I]])
// A64-NEXT:    call void @use_size_t(i64 [[TMP9]]) #5
// A64-NEXT:    [[TMP10:%.*]] = call i64 @llvm.cheri.representable.alignment.mask.i64(i64 [[I]])
// A64-NEXT:    call void @use_size_t(i64 [[TMP10]]) #5
// A64-NEXT:    [[TMP11:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* [[CAP]], i64 [[I]])
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP11]]) #5
// A64-NEXT:    [[TMP12:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.exact.i64(i8 addrspace(200)* [[CAP]], i64 [[I]])
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP12]]) #5
// A64-NEXT:    [[TMP13:%.*]] = call i64 @llvm.cheri.cap.type.get.i64(i8 addrspace(200)* [[CAP]])
// A64-NEXT:    call void @use_size_t(i64 [[TMP13]]) #5
// A64-NEXT:    [[TMP14:%.*]] = call i1 @llvm.cheri.cap.sealed.get(i8 addrspace(200)* [[CAP]])
// A64-NEXT:    call void @use_bool(i1 [[TMP14]]) #5
// A64-NEXT:    [[LNOT1:%.*]] = xor i1 [[TMP14]], true
// A64-NEXT:    call void @use_bool(i1 [[LNOT1]]) #5
// A64-NEXT:    [[CMP:%.*]] = icmp eq i64 [[TMP13]], 1
// A64-NEXT:    call void @use_bool(i1 [[CMP]]) #5
// A64-NEXT:    [[TMP15:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.seal.entry(i8 addrspace(200)* [[CAP]])
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP15]]) #5
// A64-NEXT:    [[TMP16:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.seal(i8 addrspace(200)* [[CAP]], i8 addrspace(200)* [[CAP2]])
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP16]]) #5
// A64-NEXT:    [[TMP17:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.unseal(i8 addrspace(200)* [[CAP]], i8 addrspace(200)* [[CAP2]])
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP17]]) #5
// A64-NEXT:    [[TMP18:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.build(i8 addrspace(200)* [[CAP]], i8 addrspace(200)* [[CAP2]])
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP18]]) #5
// A64-NEXT:    [[TMP19:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.conditional.seal(i8 addrspace(200)* [[CAP]], i8 addrspace(200)* [[CAP2]])
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP19]]) #5
// A64-NEXT:    [[TMP20:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.type.copy(i8 addrspace(200)* [[CAP]], i8 addrspace(200)* [[CAP2]])
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP20]]) #5
// A64-NEXT:    [[TMP21:%.*]] = call i64 @llvm.cheri.cap.perms.get.i64(i8 addrspace(200)* [[CAP]])
// A64-NEXT:    [[CONV2:%.*]] = and i64 [[TMP21]], 4294967295
// A64-NEXT:    call void @use_size_t(i64 [[CONV2]]) #5
// A64-NEXT:    [[TMP22:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.perms.and.i64(i8 addrspace(200)* [[CAP]], i64 131072)
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP22]]) #5
// A64-NEXT:    [[TMP23:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.perms.and.i64(i8 addrspace(200)* [[CAP]], i64 -32769)
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP23]]) #5
// A64-NEXT:    [[TMP24:%.*]] = call i8 addrspace(200)* @llvm.cheri.ddc.get()
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP24]]) #5
// A64-NEXT:    [[TMP25:%.*]] = call i8 addrspace(200)* @llvm.cheri.pcc.get()
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP25]]) #5
// A64-NEXT:    [[TMP26:%.*]] = call i64 @llvm.cheri.cap.flags.get.i64(i8 addrspace(200)* [[CAP]])
// A64-NEXT:    call void @use_size_t(i64 [[TMP26]]) #5
// A64-NEXT:    [[TMP27:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.flags.set.i64(i8 addrspace(200)* [[CAP]], i64 [[I]])
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP27]]) #5
// A64-NEXT:    [[TMP28:%.*]] = call i64 @llvm.cheri.cap.load.tags.i64.p0i8(i8* [[PTR:%.*]])
// A64-NEXT:    call void @use_size_t(i64 [[TMP28]]) #5
// A64-NEXT:    ret void
//
// C64-LABEL: define {{[^@]+}}@test
// C64-NEXT:  entry:
// C64-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CAP:%.*]])
// C64-NEXT:    call void @use_size_t(i64 [[TMP0]]) #5
// C64-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CAP]], i64 [[I:%.*]])
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP1]]) #5
// C64-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.base.get.i64(i8 addrspace(200)* [[CAP]])
// C64-NEXT:    call void @use_size_t(i64 [[TMP2]]) #5
// C64-NEXT:    [[TMP3:%.*]] = call i64 @llvm.cheri.cap.length.get.i64(i8 addrspace(200)* [[CAP]])
// C64-NEXT:    call void @use_size_t(i64 [[TMP3]]) #5
// C64-NEXT:    [[TMP4:%.*]] = call i64 @llvm.cheri.cap.offset.get.i64(i8 addrspace(200)* [[CAP]])
// C64-NEXT:    call void @use_size_t(i64 [[TMP4]]) #5
// C64-NEXT:    [[TMP5:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* [[CAP]], i64 [[I]])
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP5]]) #5
// C64-NEXT:    [[TMP6:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.tag.clear(i8 addrspace(200)* [[CAP]])
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP6]]) #5
// C64-NEXT:    [[TMP7:%.*]] = call i1 @llvm.cheri.cap.tag.get(i8 addrspace(200)* [[CAP]])
// C64-NEXT:    call void @use_bool(i1 [[TMP7]]) #5
// C64-NEXT:    call void @use_bool(i1 [[TMP7]]) #5
// C64-NEXT:    [[LNOT:%.*]] = xor i1 [[TMP7]], true
// C64-NEXT:    call void @use_bool(i1 [[LNOT]]) #5
// C64-NEXT:    [[TMP8:%.*]] = call i1 @llvm.cheri.cap.subset.test(i8 addrspace(200)* [[CAP]], i8 addrspace(200)* [[CAP2:%.*]])
// C64-NEXT:    call void @use_bool(i1 [[TMP8]]) #5
// C64-NEXT:    [[TMP9:%.*]] = call i64 @llvm.cheri.round.representable.length.i64(i64 [[I]])
// C64-NEXT:    call void @use_size_t(i64 [[TMP9]]) #5
// C64-NEXT:    [[TMP10:%.*]] = call i64 @llvm.cheri.representable.alignment.mask.i64(i64 [[I]])
// C64-NEXT:    call void @use_size_t(i64 [[TMP10]]) #5
// C64-NEXT:    [[TMP11:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* [[CAP]], i64 [[I]])
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP11]]) #5
// C64-NEXT:    [[TMP12:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.exact.i64(i8 addrspace(200)* [[CAP]], i64 [[I]])
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP12]]) #5
// C64-NEXT:    [[TMP13:%.*]] = call i64 @llvm.cheri.cap.type.get.i64(i8 addrspace(200)* [[CAP]])
// C64-NEXT:    call void @use_size_t(i64 [[TMP13]]) #5
// C64-NEXT:    [[TMP14:%.*]] = call i1 @llvm.cheri.cap.sealed.get(i8 addrspace(200)* [[CAP]])
// C64-NEXT:    call void @use_bool(i1 [[TMP14]]) #5
// C64-NEXT:    [[LNOT1:%.*]] = xor i1 [[TMP14]], true
// C64-NEXT:    call void @use_bool(i1 [[LNOT1]]) #5
// C64-NEXT:    [[CMP:%.*]] = icmp eq i64 [[TMP13]], 1
// C64-NEXT:    call void @use_bool(i1 [[CMP]]) #5
// C64-NEXT:    [[TMP15:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.seal.entry(i8 addrspace(200)* [[CAP]])
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP15]]) #5
// C64-NEXT:    [[TMP16:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.seal(i8 addrspace(200)* [[CAP]], i8 addrspace(200)* [[CAP2]])
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP16]]) #5
// C64-NEXT:    [[TMP17:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.unseal(i8 addrspace(200)* [[CAP]], i8 addrspace(200)* [[CAP2]])
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP17]]) #5
// C64-NEXT:    [[TMP18:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.build(i8 addrspace(200)* [[CAP]], i8 addrspace(200)* [[CAP2]])
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP18]]) #5
// C64-NEXT:    [[TMP19:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.conditional.seal(i8 addrspace(200)* [[CAP]], i8 addrspace(200)* [[CAP2]])
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP19]]) #5
// C64-NEXT:    [[TMP20:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.type.copy(i8 addrspace(200)* [[CAP]], i8 addrspace(200)* [[CAP2]])
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP20]]) #5
// C64-NEXT:    [[TMP21:%.*]] = call i64 @llvm.cheri.cap.perms.get.i64(i8 addrspace(200)* [[CAP]])
// C64-NEXT:    [[CONV2:%.*]] = and i64 [[TMP21]], 4294967295
// C64-NEXT:    call void @use_size_t(i64 [[CONV2]]) #5
// C64-NEXT:    [[TMP22:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.perms.and.i64(i8 addrspace(200)* [[CAP]], i64 131072)
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP22]]) #5
// C64-NEXT:    [[TMP23:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.perms.and.i64(i8 addrspace(200)* [[CAP]], i64 -32769)
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP23]]) #5
// C64-NEXT:    [[TMP24:%.*]] = call i8 addrspace(200)* @llvm.cheri.ddc.get()
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP24]]) #5
// C64-NEXT:    [[TMP25:%.*]] = call i8 addrspace(200)* @llvm.cheri.pcc.get()
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP25]]) #5
// C64-NEXT:    [[TMP26:%.*]] = call i64 @llvm.cheri.cap.flags.get.i64(i8 addrspace(200)* [[CAP]])
// C64-NEXT:    call void @use_size_t(i64 [[TMP26]]) #5
// C64-NEXT:    [[TMP27:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.flags.set.i64(i8 addrspace(200)* [[CAP]], i64 [[I]])
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[TMP27]]) #5
// C64-NEXT:    [[TMP28:%.*]] = call i64 @llvm.cheri.cap.load.tags.i64.p200i8(i8 addrspace(200)* [[PTR:%.*]])
// C64-NEXT:    call void @use_size_t(i64 [[TMP28]]) #5
// C64-NEXT:    ret void
//
void test(void *__capability cap, char *__capability cap2, void *ptr, __SIZE_TYPE__ i) {
  use_size_t(cheri_address_get(cap));
  use_cap(cheri_address_set(cap, i));

  use_size_t(cheri_base_get(cap));

  use_size_t(cheri_length_get(cap));

  use_size_t(cheri_offset_get(cap));
  use_cap(cheri_offset_set(cap, i));

  use_cap(cheri_tag_clear(cap));
  use_bool(cheri_tag_get(cap));
  use_bool(cheri_is_valid(cap));
  use_bool(cheri_is_invalid(cap));

  use_bool(cheri_is_subset(cap, cap2));

  use_size_t(cheri_representable_length(i));
  use_size_t(cheri_representable_alignment_mask(i));

  use_cap(cheri_bounds_set(cap, i));
  use_cap(cheri_bounds_set_exact(cap, i));

  /* Check that the cheri_otype_t type is defined */
  _Static_assert(__builtin_types_compatible_p(cheri_otype_t *, __typeof__(cheri_type_get(cap)) *), "");
  _Static_assert(__builtin_types_compatible_p(cheri_otype_t *, long *), "");
  _Static_assert(CHERI_OTYPE_UNSEALED == 0, "Unsealed type shoud be 0");
  _Static_assert(CHERI_OTYPE_SENTRY == 1, "Sealed entry type should be 1");
  use_size_t((unsigned long)cheri_type_get(cap));
  use_bool(cheri_is_sealed(cap));
  use_bool(cheri_is_unsealed(cap));
  use_bool(cheri_is_sentry(cap));
  use_cap(cheri_sentry_create(cap));
  use_cap(cheri_seal(cap, cap2));
  use_cap(cheri_unseal(cap, cap2));

  use_cap(cheri_cap_build(cap, cap2));
  use_cap(cheri_seal_conditionally(cap, cap2));
  use_cap(cheri_type_copy(cap, cap2));

  _Static_assert(CHERI_PERM_GLOBAL != 0, "must be defined");
  _Static_assert(CHERI_PERM_EXECUTE != 0, "must be defined");
  _Static_assert(CHERI_PERM_LOAD != 0, "must be defined");
  _Static_assert(CHERI_PERM_STORE != 0, "must be defined");
  _Static_assert(CHERI_PERM_LOAD_CAP != 0, "must be defined");
  _Static_assert(CHERI_PERM_STORE_CAP != 0, "must be defined");
  _Static_assert(CHERI_PERM_STORE_LOCAL_CAP != 0, "must be defined");
  _Static_assert(CHERI_PERM_SEAL != 0, "must be defined");
  _Static_assert(CHERI_PERM_UNSEAL != 0, "must be defined");
  _Static_assert(CHERI_PERM_SYSTEM_REGS != 0, "must be defined");

  _Static_assert(ARM_CAP_PERMISSION_EXECUTIVE != 0, "must be defined");
  _Static_assert(ARM_CAP_PERMISSION_MUTABLE_LOAD != 0, "must be defined");
  _Static_assert(ARM_CAP_PERMISSION_COMPARTMENT_ID != 0, "must be defined");
  _Static_assert(ARM_CAP_PERMISSION_BRANCH_SEALED_PAIR != 0, "must be defined");

  /* Check that CHERI_PERMS_T is defined */
  cheri_perms_t cap_perms = cheri_perms_get(cap);
  use_size_t(cap_perms);
  use_cap(cheri_perms_and(cap, CHERI_PERM_LOAD));
  use_cap(cheri_perms_clear(cap, CHERI_PERM_EXECUTE));

  use_cap(cheri_ddc_get());
  use_cap(cheri_pcc_get());

  use_size_t(cheri_flags_get(cap));
  use_cap(cheri_flags_set(cap, i));

  use_size_t(cheri_tags_load(ptr));
}

/* We also define macros for __builtin_is_aligned/__builtin_align_{up,down}().
 * They are not CHERI specific, but using __builtin_* is ugly.
 *
 * TOOD: we may want to provide nicer names in stdalign.h.
 */
// A64-LABEL: @test_alignment_builtins(
// A64-NEXT:  entry:
// A64-NEXT:    [[MASK:%.*]] = add i64 [[ALIGN:%.*]], -1
// A64-NEXT:    [[PTRADDR:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CAP:%.*]])
// A64-NEXT:    [[OVER_BOUNDARY:%.*]] = add i64 [[PTRADDR]], [[MASK]]
// A64-NEXT:    [[INVERTED_MASK:%.*]] = sub i64 0, [[ALIGN]]
// A64-NEXT:    [[ALIGNED_INTPTR:%.*]] = and i64 [[OVER_BOUNDARY]], [[INVERTED_MASK]]
// A64-NEXT:    [[DIFF:%.*]] = sub i64 [[ALIGNED_INTPTR]], [[PTRADDR]]
// A64-NEXT:    [[ALIGNED_RESULT:%.*]] = getelementptr inbounds i8, i8 addrspace(200)* [[CAP]], i64 [[DIFF]]
// A64-NEXT:    [[PTRINT:%.*]] = ptrtoint i8 addrspace(200)* [[ALIGNED_RESULT]] to i64
// A64-NEXT:    [[MASKEDPTR:%.*]] = and i64 [[MASK]], [[PTRINT]]
// A64-NEXT:    [[MASKCOND:%.*]] = icmp eq i64 [[MASKEDPTR]], 0
// A64-NEXT:    call void @llvm.assume(i1 [[MASKCOND]])
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[ALIGNED_RESULT]]) #5
// A64-NEXT:    [[ALIGNED_INTPTR6:%.*]] = and i64 [[PTRADDR]], [[INVERTED_MASK]]
// A64-NEXT:    [[DIFF7:%.*]] = sub i64 [[ALIGNED_INTPTR6]], [[PTRADDR]]
// A64-NEXT:    [[ALIGNED_RESULT8:%.*]] = getelementptr inbounds i8, i8 addrspace(200)* [[CAP]], i64 [[DIFF7]]
// A64-NEXT:    [[PTRINT10:%.*]] = ptrtoint i8 addrspace(200)* [[ALIGNED_RESULT8]] to i64
// A64-NEXT:    [[MASKEDPTR11:%.*]] = and i64 [[MASK]], [[PTRINT10]]
// A64-NEXT:    [[MASKCOND12:%.*]] = icmp eq i64 [[MASKEDPTR11]], 0
// A64-NEXT:    call void @llvm.assume(i1 [[MASKCOND12]])
// A64-NEXT:    call void @use_cap(i8 addrspace(200)* [[ALIGNED_RESULT8]]) #5
// A64-NEXT:    [[SET_BITS:%.*]] = and i64 [[PTRADDR]], [[MASK]]
// A64-NEXT:    [[IS_ALIGNED:%.*]] = icmp eq i64 [[SET_BITS]], 0
// A64-NEXT:    call void @use_bool(i1 [[IS_ALIGNED]]) #5
// A64-NEXT:    ret void
//
// C64-LABEL: @test_alignment_builtins(
// C64-NEXT:  entry:
// C64-NEXT:    [[MASK:%.*]] = add i64 [[ALIGN:%.*]], -1
// C64-NEXT:    [[PTRADDR:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CAP:%.*]])
// C64-NEXT:    [[OVER_BOUNDARY:%.*]] = add i64 [[PTRADDR]], [[MASK]]
// C64-NEXT:    [[INVERTED_MASK:%.*]] = sub i64 0, [[ALIGN]]
// C64-NEXT:    [[ALIGNED_INTPTR:%.*]] = and i64 [[OVER_BOUNDARY]], [[INVERTED_MASK]]
// C64-NEXT:    [[DIFF:%.*]] = sub i64 [[ALIGNED_INTPTR]], [[PTRADDR]]
// C64-NEXT:    [[ALIGNED_RESULT:%.*]] = getelementptr inbounds i8, i8 addrspace(200)* [[CAP]], i64 [[DIFF]]
// C64-NEXT:    [[PTRINT:%.*]] = ptrtoint i8 addrspace(200)* [[ALIGNED_RESULT]] to i64
// C64-NEXT:    [[MASKEDPTR:%.*]] = and i64 [[MASK]], [[PTRINT]]
// C64-NEXT:    [[MASKCOND:%.*]] = icmp eq i64 [[MASKEDPTR]], 0
// C64-NEXT:    call void @llvm.assume(i1 [[MASKCOND]])
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[ALIGNED_RESULT]]) #5
// C64-NEXT:    [[ALIGNED_INTPTR6:%.*]] = and i64 [[PTRADDR]], [[INVERTED_MASK]]
// C64-NEXT:    [[DIFF7:%.*]] = sub i64 [[ALIGNED_INTPTR6]], [[PTRADDR]]
// C64-NEXT:    [[ALIGNED_RESULT8:%.*]] = getelementptr inbounds i8, i8 addrspace(200)* [[CAP]], i64 [[DIFF7]]
// C64-NEXT:    [[PTRINT10:%.*]] = ptrtoint i8 addrspace(200)* [[ALIGNED_RESULT8]] to i64
// C64-NEXT:    [[MASKEDPTR11:%.*]] = and i64 [[MASK]], [[PTRINT10]]
// C64-NEXT:    [[MASKCOND12:%.*]] = icmp eq i64 [[MASKEDPTR11]], 0
// C64-NEXT:    call void @llvm.assume(i1 [[MASKCOND12]])
// C64-NEXT:    call void @use_cap(i8 addrspace(200)* [[ALIGNED_RESULT8]]) #5
// C64-NEXT:    [[SET_BITS:%.*]] = and i64 [[PTRADDR]], [[MASK]]
// C64-NEXT:    [[IS_ALIGNED:%.*]] = icmp eq i64 [[SET_BITS]], 0
// C64-NEXT:    call void @use_bool(i1 [[IS_ALIGNED]]) #5
// C64-NEXT:    ret void
//
void test_alignment_builtins(void *__capability cap, __SIZE_TYPE__ align) {
  use_cap(cheri_align_up(cap, align));
  use_cap(cheri_align_down(cap, align));
  use_bool(cheri_is_aligned(cap, align));
}
